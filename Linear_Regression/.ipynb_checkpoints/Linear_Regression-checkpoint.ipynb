{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In this notebook, we will go through the linear regression algorithms to estimate the price of houses in Boston.\n",
    "\n",
    "First, it is shown how to compute the predictions using a single feature. Later on, the predictions are expanded for multiple features. In both cases, I will compare my predictions with the Turicreate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with a single feature\n",
    "\n",
    "In the first part, let's run the Turicreate linear regression algorithm to understand is functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mguarido/anaconda2/envs/venv/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/mguarido/anaconda2/envs/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import turicreate as tc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tc.SFrame('BostonHousing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.random_split(0.8, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a model using the turicreate.linear_regression.create using the living square feet size as the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tc.linear_regression.create(train_data, target='price', features = ['sqft_living'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_data['sqft_living'], test_data['price']/1000000,'.',\n",
    "         test_data['sqft_living'], predictions/1000000,'-')\n",
    "plt.ylabel('Price (Millions USD$)')\n",
    "plt.xlabel('House Size (Square Feet)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a prediction for a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_to_predict = 15000\n",
    "forecast = model.predict(tc.SFrame({'sqft_living':[size_to_predict]}))\n",
    "print forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now, let's create our own function\n",
    "\n",
    "We want to fit a line that best estimates all the values in the training set. For that, we need to find 2 parameters: the slope and the intercept. The equation of the line is:\n",
    "\n",
    "$$y_{i}^{,}(x_i) = w_0 + w_{1}*x_i$$\n",
    "\n",
    "Now, we want to find the parameters $w_0$ and $w_1$ that reduces the cost function (the sum of the squared difference between measured data $y_i$ to the predicted data $y_{i}^{,}$):\n",
    "\n",
    "$$RSS(w_0,w_1) = \\sum_{i=1}^{N}(y_i - y_{i}^{,})^2 = \\sum_{i=1}^{N}(y_i - [w_0 + w_{1}*x_i])^2$$\n",
    "\n",
    "Minimizing the cost function means to take the derivative of cost function for each parameter ($w_0$ and $w_1$) and make it equal to zero. This leads to 2 simple formulas for $w_0$ and $w_1$:\n",
    "\n",
    "$$w_0 = \\frac{\\sum_{i=1}^{N}y_i}{N} - w_{1}\\frac{\\sum_{i=1}^{N}x_i}{N}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$w_1 = \\frac{\\sum_{i=1}^{N}y_{i}x_{i} - \\frac{\\sum_{i=1}^{N}y_{i}\\sum_{i=1}^{N}x_{i}}{N}}{\\sum_{i=1}^{N}x_{i}^2 - \\frac{\\sum_{i=1}^{N}x_{i}\\sum_{i=1}^{N}x_{i}}{N}}$$\n",
    "\n",
    "With the equations above, it is possible to compute the intercept ($w_0$) and the slope ($w_1$) that best predict the output $(y_{i}^{'})$ given the input $x_i$ and the measured data $y_i$ (for one feature only).\n",
    "\n",
    "Now, let's create the function that gets the input feature $x_i$ and the measured data $y_i$ of the training set, and return the intercept $w_0$ and the slope $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_single(input_feature, measured_data):\n",
    "    # First, let's compute the sums and squared sums of the parameters equations\n",
    "    Isum = input_feature.sum()\n",
    "    Msum = measured_data.sum()\n",
    "    IMsum = sum([input_feature[i]*measured_data[i] for i in range(len(input_feature))])\n",
    "    IIsum = sum([input_feature[i]*input_feature[i] for i in range(len(input_feature))])\n",
    "\n",
    "    # We need to compute the slope first\n",
    "    num = IMsum-(1./len(input_feature)*(Isum*Msum))\n",
    "    den = IIsum-(1./len(input_feature)*(Isum*Isum))\n",
    "    slope = num/den\n",
    "    \n",
    "    # Now that we have the slope, we can compute the intercept\n",
    "    intercept = (1./len(input_feature))*Msum-slope*Isum*(1./len(input_feature))\n",
    "    \n",
    "    # Return the parameters\n",
    "    return (intercept, slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_1, slope_1 = linear_regression_single(train_data['sqft_living'], train_data['price'])\n",
    "\n",
    "print \"Intercept: \" + str(intercept_1)\n",
    "print \"Slope: \" + str(slope_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those values look close to the ones obtained by using the Turicreate.\n",
    "\n",
    "The next step is to calculate the estimations $y_{i}^{,}$ by using the linear equation:\n",
    "\n",
    "$$y_{i}^{,}(x_i) = w_0 + w_{1}*x_i$$\n",
    "\n",
    "Let's create a function that gets the input feature $x_i$ and the measured data $y_i$. It needs to estimate the intercept $w_0$ and slope $w_1$ and calculate the predictions $y_{i}^{,}$ using the linear equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_predictions_single(input_feature, measured_data, single_input = 0):\n",
    "    # First, we need to estimete the intercept and the slope\n",
    "    intercept, slope = linear_regression_single(input_feature, measured_data)\n",
    "    \n",
    "    # Now, compute the predictions\n",
    "    predicted_values = intercept + slope*input_feature\n",
    "    \n",
    "    # Computing single prediction to compare functions\n",
    "    single_prediction = 0\n",
    "    if single_input != 0:\n",
    "        single_prediction = intercept + slope*single_input\n",
    "        \n",
    "    # Return outputs\n",
    "    return predicted_values, single_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function has the input feature, slope and intercept as input so the can estimate the parameters in the training data and make predictions in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_predictions_single_test(input_feature, slope, intercept, single_input = 0):\n",
    "    # In this functions, we input the slope and intercept, so we can make predictions on the testing set\n",
    "    predicted_values = intercept + slope*input_feature\n",
    "\n",
    "    # Computing single prediction to compare functions\n",
    "    single_prediction = 0\n",
    "    if single_input != 0:\n",
    "        single_prediction = intercept + slope*single_input\n",
    "        \n",
    "    # Return outputs\n",
    "    return predicted_values, single_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, single_prediction = regression_predictions_single(data['sqft_living'], data['price'], size_to_predict)\n",
    "\n",
    "plt.plot(data['sqft_living'], data['price']/1000000,'.',\n",
    "         data['sqft_living'], predictions/1000000,'-')\n",
    "plt.ylabel('Price (Millions USD$)')\n",
    "plt.xlabel('House Size (Square Feet)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we did a linear regression for the whole dataset. Below, we are using the slope and intercept estimated in the training data to make estimations over the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2, single_prediction2 = regression_predictions_single_test(test_data['sqft_living'], slope_1, intercept_1, size_to_predict)\n",
    "\n",
    "plt.plot(test_data['sqft_living'], test_data['price']/1000000,'.',\n",
    "         test_data['sqft_living'], predictions2/1000000,'-')\n",
    "plt.ylabel('Price (Millions USD$)')\n",
    "plt.xlabel('House Size (Square Feet)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print single_prediction2\n",
    "print forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is very similar to the estimation we get with the Turicreate. But we did all for only a single feature. How do we proceed having multiple features? But before going for multiple features, we need to evaluate our predictions. One way to evaluate the prediction model, is to calculate the **_Residual sum of squares_** (RSS).\n",
    "\n",
    "$$RSS(w_0,w_1) = \\sum_{i=1}^{N}(y_i - [w_0 + w_{1}*x_i])^2$$\n",
    "\n",
    "Let's create the function that computes the RSS by inputing the predictions $y_{i}^{,}$ and the measured data $y_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RSS(predictions,measured_data):\n",
    "    # First, let's calculate the residuals (difference between measured and predictated data)\n",
    "    residuals = measured_data - predictions\n",
    "    \n",
    "    # Compute RSS\n",
    "    RSS = sum([residuals[i]*residuals[i] for i in range(len(residuals))])\n",
    "    \n",
    "    # Return RSS\n",
    "    return(RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_test = calculate_RSS(predictions2,test_data['price'])\n",
    "\n",
    "print rss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now it is time to predict the house prices using multiple features. First, let's demonstrate how to do that using the Turicreate. Basically, it follows the same steps as the linear regression with a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tc.SFrame('BostonHousing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.random_split(0.8, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'yr_built', 'condition']\n",
    "model_multiple = tc.linear_regression.create(train_data, target='price', features = multiple_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_multiple = model_multiple.predict(test_data)\n",
    "results_multiple = model_multiple.evaluate(test_data)\n",
    "\n",
    "print results_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_to_predict = 15000\n",
    "forecast_multiple = model_multiple.predict(tc.SFrame({'sqft_living':[size_to_predict]}))\n",
    "print forecast_multiple\n",
    "print forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the the forecast for a house with 15000 sqrtfeet living size changed when we compare the multiple features predictions with the single feature predictions.\n",
    "\n",
    "If we plot the predictions comparing with the testing data, we can observe that the predictions are not a single line anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_data['sqft_living'], test_data['price']/1000000,'.',\n",
    "         test_data['sqft_living'], predictions_multiple/1000000,'-')\n",
    "plt.ylabel('Price (Millions USD$)')\n",
    "plt.xlabel('House Size (Square Feet)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build our own functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Until now, I applied the linear regression over one single feature. This way, it was easy to find an analytical solution for the linear equation. Now, I will explore the use of several features and an analytical solution is not recommended. So, I will show how to use the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) method to fit the best \"curve\". The idea is to use an iterative strategy to find the best parameters w that leads to the lowest value between the difference of the predictions and the actual data.\n",
    "\n",
    "The \"linear\" equation for multiple parameters (now a multi-dimensinal plane) can be written as:\n",
    "\n",
    "$$y_{i}^{,}(x_i) = w_0*x_{i}[0] + w_{1}*x_{i}[1] + w_{2}*x_{i}[2] + \\dots + w_{d}*x_{i}[d] = \\sum_{j=0}^{d}w_j*x_{i}[j]$$\n",
    "\n",
    "where $x_i[0]=1$. We can use matrix notation and use the dot product of the parameters $\\textbf{w}$ with the features $\\textbf{x}_i$:\n",
    "\n",
    "$$y_{i}^{,}(\\textbf{x}_i) = \\textbf{w}^{T}\\textbf{x}_i$$\n",
    "\n",
    "The cost function (RSS) is:\n",
    "\n",
    "$$RSS(\\textbf{w}) = \\sum_{i=1}^{N}(y_i - y_{i}^{,})^2 = \\sum_{i=1}^{N}(y_i - \\textbf{w}^{T}\\textbf{x}_i)^2 = (\\textbf{y} - \\textbf{w}^{T}\\textbf{x})^{T}(\\textbf{y} - \\textbf{w}^{T}\\textbf{x})$$\n",
    "\n",
    "The gradient $\\Phi$ (the derivative of RSS relative to $\\textbf{w}$) is:\n",
    "\n",
    "$$\\Phi = -2\\textbf{x}^{T}(\\textbf{y} - \\textbf{w}^{T}\\textbf{x})$$\n",
    "\n",
    "In others words, we just need to take the dot product between the features and the residuals (the difference between the predictions and the measured data) and multiply by 2. The gradient is used to update the parameters $\\textbf{w}$ iteratively in a way to minimize the errors (set the gradient to zero). In the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) method, for the n-th iteration, we end up with the following equation:\n",
    "\n",
    "$$\\textbf{w}_{n+1} = \\textbf{w}_{n} - \\alpha\\Phi = \\textbf{w}_{n} + 2{\\alpha}\\textbf{x}^{T}(\\textbf{y} - \\textbf{w}^{T}_{n}\\textbf{x})$$\n",
    "\n",
    "where $\\alpha$ is the step length, a scalar that helps the minimization to converge faster to the minimum point. The choice of the step length shows to be trick, as a large value wil lead to minimization to diverge as a too small value will diverge slowly.\n",
    "\n",
    "Now we shall be able to compute the best set of parameters given a set of features and the output target. But to compute this, first we need to convert the data in the sframe format (the Turicreate dataframe) to a numpy numerical matrix. We will create a series of functions to combine in a final one that will calculate the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function gets the sframe dataset and the names of the features and the target output to be converted to a numerical matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, output):\n",
    "    # First, create a column with the feature w_0 = 1\n",
    "    data_sframe['constant'] = 1\n",
    "\n",
    "    # Include the w_0 in the features list in the first column\n",
    "    features = ['constant'] + features \n",
    "    \n",
    "    # From the sframe data, pick only desired features and convert to numerical matrices\n",
    "    feature_matrix = data_sframe[features].to_numpy()\n",
    "    \n",
    "    # Doing the same for the output\n",
    "    output_array = data_sframe[output].to_numpy()\n",
    "    \n",
    "    # Return the features and output target as numerical matrices\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(example_features, example_output) = get_numpy_data(data, ['sqft_living'], 'price') # the [] around 'sqft_living' makes it a list\n",
    "print example_features[0,:] \n",
    "print example_output[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make our life easier, we can use the fact that the predictions are simply the dot product of the parameters with the features to create a function to compute the prediction given the matrix of the features and the array of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # Th predictions are the dot product of the features and weights (parameters)\n",
    "    predictions = np.dot(feature_matrix,weights)\n",
    "    \n",
    "    # Return the predictions\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First computing manually\n",
    "my_weights = np.array([1., 1.]) # the example weights\n",
    "my_features = example_features[0,] # we'll use the first data point\n",
    "predicted_value = np.dot(my_features, my_weights)\n",
    "\n",
    "# Now using our function\n",
    "test_predictions = predict_output(example_features, my_weights)\n",
    "\n",
    "print predicted_value\n",
    "print test_predictions[0]\n",
    "print test_predictions[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we are almost there. Now, let's use the equation forthe gradient to create a function to estimate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(residuals, feature):\n",
    "    # Using the equation of the derivative,the dop product between the features and residuals\n",
    "    derivative = 2*np.dot(residuals,feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only on feature\n",
    "(example_features, example_output) = get_numpy_data(data, ['sqft_living'], 'price') \n",
    "\n",
    "# Making the parameters = 0 so the derivative should be\n",
    "# equal the sum of the output. This is an easy way to\n",
    "# evaluate our function\n",
    "my_weights = np.array([0., 0.])\n",
    "\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "\n",
    "residuals = test_predictions - example_output\n",
    "\n",
    "# Let's compute the derivative with respect to 'constant',\n",
    "# the \":\" indicates \"all rows\"\n",
    "features = example_features[:,0] \n",
    "\n",
    "derivative = feature_derivative(residuals, features)\n",
    "\n",
    "print derivative\n",
    "print -np.sum(example_output)*2 # should be the same as derivative\n",
    "print derivative - (-np.sum(example_output)*2) # should be ZERO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now, to the gradient descent method. We need to define a stoping criteria (tolerace), a defined value that stops the loop if the gradient is smaller, the initial parameters (weights) guess, and the step length (set as constant for all the iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    iteration = 0\n",
    "    \n",
    "    while not converged:\n",
    "        # Compitung predictions with current weights\n",
    "        predictions = predict_output(feature_matrix, weights) \n",
    "        \n",
    "        # Computing current residuals\n",
    "        residuals = predictions-output\n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        \n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Estimating the derivative for each parameter (weight)\n",
    "            derivative = feature_derivative(residuals,feature_matrix[:, i])\n",
    "            \n",
    "            # add the squared value of the derivative to the gradient magnitude (for assessing convergence)\n",
    "            gradient_sum_squares = gradient_sum_squares + (derivative*derivative)\n",
    "            \n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            weights[i] = weights[i]-step_size*derivative\n",
    "        \n",
    "        # compute the square-root of the gradient sum of squares to get the gradient matnigude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        iteration = iteration+1\n",
    "\n",
    "        # The tolerance is our stoping criteria\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    \n",
    "    # Return the parameters (weights) and number of iterations\n",
    "    return(weights,iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the function over the train data\n",
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9\n",
    "\n",
    "# Gradient descent\n",
    "(weights,iteration) = regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "\n",
    "# Predictions over the test data\n",
    "(test_feature_matrix, test_output_new) = get_numpy_data(test_data, model_features, my_output)\n",
    "prediction = predict_output(test_feature_matrix, weights)\n",
    "\n",
    "# Checking outputs\n",
    "print weights\n",
    "print iteration\n",
    "print prediction[0]\n",
    "print output[0]\n",
    "print prediction[0] - output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To use features with different number sizer (like the house sqrtft size and number of bedrooms), we need to [scale the features](https://en.wikipedia.org/wiki/Feature_scaling) to have them all in the same scale, but we are not applying it now.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
